# -*- coding: utf-8 -*-
"""Bioinformatics_1ext_all_possible_testings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PCCKDwfoulNv7urhUYBHL2hU9xkOn8hL
"""

!pip install chembl_webresource_client

import pandas as pd
from chembl_webresource_client.new_client import new_client

#targeting for coronavirus
target = new_client.target
target_query = target.search('coronavirus')
# Corrected the capitalization of DataFrame
targets = pd.DataFrame.from_dict(target_query)
targets

single_proteins = targets[targets['target_type'] == 'SINGLE PROTEIN']
single_proteins

selected_target1 = targets.target_chembl_id[6]
selected_target2 = targets.target_chembl_id[8]
selected_target3 = targets.target_chembl_id[9]
selected_target1

activity = new_client.activity
res1 = activity.filter(target_chembl_id=selected_target1).filter(standard_type="IC50")
#res1 = activity.filter(target_chembl_id=selected_target2).filter(standard_type="IC50")
#res1 = activity.filter(target_chembl_id=selected_target3).filter(standard_type="IC50")

df = pd.DataFrame.from_dict(res1)

df.head(3)

df.standard_value
df[['standard_value', 'assay_chembl_id', 'assay_description']].head(3)
#the lower the number the lower the concentration tp achieve the result the better

#Filter out unique identifier data types(there may be combinations of )
df.standard_type.unique()

df.to_csv('bioactivity_data.csv', index=False)

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p "/content/drive/MyDrive/Bioactivity/data1"

!cp bioactivity_data.csv "/content/drive/MyDrive/Bioactivity/data1"

!ls "/content/drive/MyDrive/Bioactivity/data1"

df2 = df[df.standard_value.notna()]
df2

bioactivity_class = []
for i in df2.standard_value:
  if float(i) >= 10000:
    bioactivity_class.append("inactive")
  elif float(i) <= 1000:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")

mol_cid = []
for i in df2.molecule_chembl_id:
  mol_cid.append(i)

canonical_smiles = []
for i in df2.canonical_smiles:
  canonical_smiles.append(i)

standard_value = []
for i in df2.standard_value:
  standard_value.append(i)

data_tuples = list(zip(mol_cid, canonical_smiles, bioactivity_class, standard_value))
df3= pd.DataFrame(data_tuples, columns = ['molecule_chembl_id', 'canonical_smiles', 'bioactivity_class', 'standard_value'])
df3

df3.to_csv('bioactivity_preprocessed_data.csv', index=False)
!cp bioactivity_preprocessed_data.csv "/content/drive/MyDrive/Bioactivity/data1"

! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local
! conda install -c rdkit rdkit -y
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

!pip install rdkit

import pandas as pd
df = pd.read_csv('bioactivity_preprocessed_data.csv')
import numpy as np
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem)
        moldata.append(mol)

    baseData= np.arange(1,1)
    i=0
    for mol in moldata:

        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)

        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors])

        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1

    columnNames=["MW","LogP","NumHDonors","NumHAcceptors"]
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)

    return descriptors

df_lipinski = lipinski(df.canonical_smiles)
df_lipinski

df

df_combined = pd.concat([df,df_lipinski], axis=1)
df_combined

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def calculate_pIC50(df_input):
    df = df_input.copy()
    df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')
    df.dropna(subset=['standard_value'], inplace=True)
    norm_values = []
    for i in df['standard_value']:
        if i > 100000000:
            i = 100000000
        norm_values.append(i)
    df['standard_value_norm'] = norm_values
    df['pIC50'] = -np.log10(df['standard_value_norm'] * (10**-9))
    df.drop(columns=['standard_value_norm'], inplace=True)
    return df

df_pIC50 = calculate_pIC50(df_combined)

print("Generating IC50 vs pIC50 plot...")

plt.figure(figsize=(10, 8))
scatterplot = sns.scatterplot(
    x='standard_value',
    y='pIC50',
    data=df_pIC50,
    hue='bioactivity_class',
    palette={'active': 'green', 'inactive': 'red', 'intermediate': 'blue'},
    edgecolor='black',
    alpha=0.7,
    s=60
)
plt.xscale('log')

plt.xlabel('IC50 (nM) - Log Scale', fontsize=14, fontweight='bold')
plt.ylabel('pIC50', fontsize=14, fontweight='bold')
plt.title('Relationship between IC50 and pIC50', fontsize=16, fontweight='bold')
plt.grid(True, which="both", ls="--")
plt.legend(title='Bioactivity Class')
plt.show()

print("\nFinal DataFrame with pIC50 column:")
print(df_pIC50[['molecule_chembl_id', 'standard_value', 'bioactivity_class', 'pIC50']].head())

df_final = df_pIC50

df_final.pIC50.describe()

df_2class = df_final[df_final.bioactivity_class != 'intermediate']
df_2class

import seaborn as sns
sns.set(style='ticks')
import matplotlib.pyplot as plt
plt.figure(figsize=(5.5, 5.5))

sns.countplot(x='bioactivity_class', data=df_2class, edgecolor='black')

plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('Frequency', fontsize=14, fontweight='bold')

plt.savefig('plot_bioactivity_class.pdf')

plt.figure(figsize=(5.5, 5.5))

sns.scatterplot(x='MW', y='LogP', data=df_2class, hue='bioactivity_class', size='pIC50', edgecolor='black', alpha=0.7)

plt.xlabel('MW', fontsize=14, fontweight='bold')
plt.ylabel('LogP', fontsize=14, fontweight='bold')
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)
plt.savefig('plot_MW_vs_LogP.pdf')

plt.figure(figsize=(5.5, 5.5))

sns.boxplot(x = 'bioactivity_class', y = 'pIC50', data = df_2class)

plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('pIC50 value', fontsize=14, fontweight='bold')

plt.savefig('plot_ic50.pdf')

def mannwhitney(descriptor, verbose=False):
  # https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/
  from numpy.random import seed
  from numpy.random import randn
  from scipy.stats import mannwhitneyu

  seed(1)

  selection = [descriptor, 'bioactivity_class']
  df = df_2class[selection]
  active = df[df.bioactivity_class == 'active']
  active = active[descriptor]

  selection = [descriptor, 'bioactivity_class']
  df = df_2class[selection]
  inactive = df[df.bioactivity_class == 'inactive']
  inactive = inactive[descriptor]

  stat, p = mannwhitneyu(active, inactive)

  alpha = 0.05
  if p > alpha:
    interpretation = 'Same distribution (fail to reject H0)'
  else:
    interpretation = 'Different distribution (reject H0)'

  results = pd.DataFrame({'Descriptor':descriptor,
                          'Statistics':stat,
                          'p':p,
                          'alpha':alpha,
                          'Interpretation':interpretation}, index=[0])
  filename = 'mannwhitneyu_' + descriptor + '.csv'
  results.to_csv(filename)

  return results

mannwhitney('pIC50')

plt.figure(figsize=(5.5, 5.5))

sns.boxplot(x = 'bioactivity_class', y = 'MW', data = df_2class)

plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('MW', fontsize=14, fontweight='bold')

plt.savefig('plot_MW.pdf')

mannwhitney('MW')

plt.figure(figsize=(5.5, 5.5))

sns.boxplot(x = 'bioactivity_class', y = 'LogP', data = df_2class)

plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('LogP', fontsize=14, fontweight='bold')

plt.savefig('plot_LogP.pdf')

mannwhitney('LogP')

plt.figure(figsize=(5.5, 5.5))

sns.boxplot(x = 'bioactivity_class', y = 'NumHDonors', data = df_2class)

plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('NumHDonors', fontsize=14, fontweight='bold')

plt.savefig('plot_NumHDonors.pdf')

mannwhitney('NumHDonors')

plt.figure(figsize=(5.5, 5.5))

sns.boxplot(x = 'bioactivity_class', y = 'NumHAcceptors', data = df_2class)

plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('NumHAcceptors', fontsize=14, fontweight='bold')

plt.savefig('plot_NumHAcceptors.pdf')

mannwhitney('NumHAcceptors')

!zip -r results.zip . -i *.csv *.pdf

"""**Lipinski's descriptors**

Of the 4 Lipinski's descriptors (MW, LogP, NumHDonors and NumHAcceptors), only LogP exhibited no difference between the actives and inactives while the other 3 descriptors (MW, NumHDonors and NumHAcceptors) shows statistically significant difference between actives and inactives.

**Enhanced Feature Engineering**

Instead of calculating just four descriptors, calculating all of them
"""

from rdkit.Chem import Descriptors
import pandas as pd
df_preprocessed = pd.read_csv('bioactivity_preprocessed_data.csv')
def calculate_all_rdkit_descriptors(smiles_list):
  descriptor_names = [name for name, func in Descriptors._descList]
  all_descriptors = []
  for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
      all_descriptors.append([None]*len(descriptor_names))
      continue
    descriptors = [func(mol) for name, func in Descriptors._descList]
    all_descriptors.append(descriptors)
  return pd.DataFrame(all_descriptors, columns=descriptor_names)
df_descriptors = calculate_all_rdkit_descriptors(df_preprocessed['canonical_smiles'])

df_descriptors = df_descriptors.dropna(axis=1, how='any') # Drop columns with any None/NaN
df_descriptors = df_descriptors.loc[:, (df_descriptors != df_descriptors.iloc[0]).any()] # Drop columns with no variance

print(f"Calculated {df_descriptors.shape[1]} descriptors.")
print(df_descriptors.head())

"""**Generating** **Molecular** **Fingerprints**

Molecular fingerprints are bit vectors where each bit represents the presence or absence of a specific chemical substructure or feature. They are one of the most powerful and widely used features for machine learning in chemistry
"""

!conda install -c conda-forge rdkit numpy pandas -y

import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

from rdkit.Chem import AllChem
import pandas as pd
from rdkit import Chem
if 'df_preprocessed' not in globals():
    try:
        df_preprocessed = pd.read_csv('bioactivity_preprocessed_data.csv')
        print("Loaded df_preprocessed from CSV.")
    except FileNotFoundError:
        print("Error: bioactivity_preprocessed_data.csv not found. Please run previous cells.")
        df_preprocessed = None


def generate_morgan_fingerprints(smiles_list, radius=2, n_bits=2048):
    fp_list = []
    for smiles in smiles_list:
        try:
            mol = Chem.MolFromSmiles(smiles)
        except Exception as e:
            print(f"Error processing SMILES '{smiles}': {e}")
            mol = None
        if mol is None:
            fp_list.append(None)
            continue
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
        fp_list.append(list(fp)) # Convert to list of 0s and 1s
    col_names = [f'morgan_{i}' for i in range(n_bits)]
    df_fp = pd.DataFrame(fp_list, columns=col_names).dropna(axis=0).reset_index(drop=True)
    return df_fp

if df_preprocessed is not None:
    df_fingerprints = generate_morgan_fingerprints(df_preprocessed['canonical_smiles'])
    print(df_fingerprints.head())
else:
    print("Fingerprint generation skipped due to missing df_preprocessed.")

""" Build Predictive Models (**QSAR Modeling**)
 Quantitative Structure-Activity Relationship model
"""

X = df_fingerprints

y = df_final['pIC50']

final_df = pd.concat([y, X], axis=1).dropna()
X = final_df.drop('pIC50', axis=1)
y = final_df['pIC50']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Model Performance on Test Set:")
print(f"R-squared (R²): {r2:.3f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.3f}")

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8, 8))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.7, edgecolor='k')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Add a line for perfect prediction
plt.xlabel("Actual pIC50", fontsize=14)
plt.ylabel("Predicted pIC50", fontsize=14)
plt.title("Model Performance: Actual vs. Predicted", fontsize=16)
plt.grid(True)
plt.show()

X = df_fingerprints
y = df_final['pIC50']

# --- Important Sanity Check ---
# The number of rows in X and y must match. Invalid SMILES might have been dropped.
# Let's combine them and drop any rows with missing data to ensure they align perfectly.
final_model_df = pd.concat([y, X], axis=1).dropna()

X = final_model_df.drop('pIC50', axis=1)
y = final_model_df['pIC50']

print(f"Shape of feature matrix (X): {X.shape}")
print(f"Shape of target vector (y): {y.shape}")

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("\nModel Performance on Test Set:")
print(f"R-squared (R²): {r2:.3f}") # Closer to 1 is better
print(f"Root Mean Squared Error (RMSE): {rmse:.3f}") # Lower is better

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8, 8))
sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.6, 'edgecolor':'k'})
plt.xlabel("Actual pIC50", fontsize=14)
plt.ylabel("Predicted pIC50", fontsize=14)
plt.title("Model Performance: Actual vs. Predicted pIC50", fontsize=16)
plt.grid(True)
plt.show()

"""# Task
Explain the provided Python code for training a RandomForestRegressor model, then modify it to use GridSearchCV and RandomizedSearchCV for hyperparameter tuning, evaluate the performance of the model trained with the best hyperparameters from each method, and compare the results with the original model.

## Define parameter grids

### Subtask:
Define the hyperparameter grids for GridSearchCV and RandomizedSearchCV.

**Reasoning**:
Define the hyperparameter grids for GridSearchCV and RandomizedSearchCV as dictionaries according to the instructions.
"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

random_param_grid = {
    'n_estimators': [100, 200, 300, 500, 800],
    'max_depth': [5, 10, 15, 20, 25, 30, None],
    'min_samples_split': [2, 5, 10, 15, 20],
    'min_samples_leaf': [1, 2, 4, 8, 10]
}

"""## Perform gridsearchcv

### Subtask:
Use GridSearchCV to search for the best hyperparameters for the RandomForestRegressor model.

**Reasoning**:
Use GridSearchCV to find the best hyperparameters for the RandomForestRegressor model.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state=42)

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)

grid_search.fit(X_train, y_train)

print("Best hyperparameters found by GridSearchCV:")
print(grid_search.best_params_)

print("\nBest cross-validation score found by GridSearchCV:")
print(grid_search.best_score_)

"""**Reasoning**:
The previous command failed because X_train and y_train were not defined in the current session. Need to reload the data and perform the train-test split again before running GridSearchCV.


"""

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
import numpy as np

if 'df_preprocessed' not in globals():
    try:
        df_preprocessed = pd.read_csv('bioactivity_preprocessed_data.csv')
        print("Loaded df_preprocessed from CSV.")
    except FileNotFoundError:
        print("Error: bioactivity_preprocessed_data.csv not found.")
        df_preprocessed = None

def generate_morgan_fingerprints(smiles_list, radius=2, n_bits=2048):
    fp_list = []
    for smiles in smiles_list:
        try:
            mol = Chem.MolFromSmiles(smiles)
        except Exception as e:
            print(f"Error processing SMILES '{smiles}': {e}")
            mol = None
        if mol is None:
            fp_list.append(None)
            continue
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
        fp_list.append(list(fp))
    col_names = [f'morgan_{i}' for i in range(n_bits)]
    df_fp = pd.DataFrame(fp_list, columns=col_names) # Don't dropna here yet
    return df_fp

if df_preprocessed is not None:
    df_fingerprints = generate_morgan_fingerprints(df_preprocessed['canonical_smiles'])

    # Ensure df_fingerprints and df_preprocessed have the same index before combining
    df_preprocessed = df_preprocessed.reset_index(drop=True)
    df_fingerprints = df_fingerprints.reset_index(drop=True)


    # Combine fingerprints with pIC50 and handle missing values
    # First, calculate pIC50 if not already in df_preprocessed
    if 'pIC50' not in df_preprocessed.columns:
        def calculate_pIC50(df_input):
            df = df_input.copy()
            df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')
            df.dropna(subset=['standard_value'], inplace=True)
            norm_values = []
            for i in df['standard_value']:
                if i > 100000000:
                    i = 100000000
                norm_values.append(i)
            df['standard_value_norm'] = norm_values
            df['pIC50'] = -np.log10(df['standard_value_norm'] * (10**-9))
            df.drop(columns=['standard_value_norm'], inplace=True)
            return df

        df_preprocessed = calculate_pIC50(df_preprocessed)
        print("Calculated pIC50.")

    # Align df_fingerprints to df_preprocessed after pIC50 calculation and dropping NaNs
    # This ensures that the fingerprints correspond to the rows that have pIC50 values
    aligned_fingerprints = df_fingerprints.loc[df_preprocessed.index].reset_index(drop=True)


    X = aligned_fingerprints
    y = df_preprocessed['pIC50']

    # Drop rows with NaN values in X after generating fingerprints
    # This handles cases where fingerprint generation might fail for a SMILES
    nan_rows_X = X.isnull().any(axis=1)
    if nan_rows_X.any():
        print(f"Dropping {nan_rows_X.sum()} rows due to NaN values in fingerprints.")
        X = X[~nan_rows_X].reset_index(drop=True)
        y = y[~nan_rows_X].reset_index(drop=True)


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}")

    rf = RandomForestRegressor(random_state=42)

    # Define param_grid - ensure it is defined if not already in scope
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }


    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)

    grid_search.fit(X_train, y_train)

    print("Best hyperparameters found by GridSearchCV:")
    print(grid_search.best_params_)

    print("\nBest cross-validation score found by GridSearchCV:")
    print(grid_search.best_score_)

else:
    print("GridSearchCV skipped due to missing data.")

"""## Evaluate gridsearchcv model

### Subtask:
Evaluate the model trained with the best hyperparameters found by GridSearchCV on the test set.

**Reasoning**:
Evaluate the model trained with the best hyperparameters found by GridSearchCV on the test set by calculating R-squared and RMSE.
"""

# Get the best estimator from GridSearchCV
best_grid_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred_grid = best_grid_model.predict(X_test)

# Calculate R-squared and RMSE
r2_grid = r2_score(y_test, y_pred_grid)
rmse_grid = np.sqrt(mean_squared_error(y_test, y_pred_grid))

# Print the results
print(f"GridSearchCV Model Performance on Test Set:")
print(f"R-squared (R²): {r2_grid:.3f}")
print(f"Root Mean Squared Error (RMSE): {rmse_grid:.3f}")

"""## Perform randomizedsearchcv

### Subtask:
Use RandomizedSearchCV to search for the best hyperparameters for the RandomForestRegressor model.

**Reasoning**:
Import necessary libraries and perform RandomizedSearchCV to find the best hyperparameters for the RandomForestRegressor model.
"""

from sklearn.model_selection import RandomizedSearchCV

rf = RandomForestRegressor(random_state=42)

random_search = RandomizedSearchCV(estimator=rf, param_distributions=random_param_grid, n_iter=10, cv=5, random_state=42, n_jobs=-1)

random_search.fit(X_train, y_train)

print("Best hyperparameters found by RandomizedSearchCV:")
print(random_search.best_params_)

print("\nBest cross-validation score found by RandomizedSearchCV:")
print(random_search.best_score_)

"""## Evaluate randomizedsearchcv model

### Subtask:
Evaluate the model trained with the best hyperparameters found by RandomizedSearchCV on the test set.

**Reasoning**:
Evaluate the model trained with the best hyperparameters found by RandomizedSearchCV on the test set by making predictions and calculating R-squared and RMSE.
"""

# Get the best estimator from RandomizedSearchCV
best_random_model = random_search.best_estimator_

# Make predictions on the test set
y_pred_random = best_random_model.predict(X_test)

# Calculate R-squared and RMSE
r2_random = r2_score(y_test, y_pred_random)
rmse_random = np.sqrt(mean_squared_error(y_test, y_pred_random))

# Print the results
print(f"RandomizedSearchCV Model Performance on Test Set:")
print(f"R-squared (R²): {r2_random:.3f}")
print(f"Root Mean Squared Error (RMSE): {rmse_random:.3f}")

"""## Compare model performance

### Subtask:
Compare the performance of the three models (current, GridSearchCV, and RandomizedSearchCV) based on R-squared and RMSE.

**Reasoning**:
Print the performance metrics for all three models (original, GridSearchCV, and RandomizedSearchCV) to compare their R-squared and RMSE values and then interpret the results.
"""

print("Performance Comparison:")
print("-" * 30)
print(f"Original Model:")
print(f"  R-squared (R²): {r2:.3f}")
print(f"  Root Mean Squared Error (RMSE): {rmse:.3f}")
print("-" * 30)
print(f"GridSearchCV Model:")
print(f"  R-squared (R²): {r2_grid:.3f}")
print(f"  Root Mean Squared Error (RMSE): {rmse_grid:.3f}")
print("-" * 30)
print(f"RandomizedSearchCV Model:")
print(f"  R-squared (R²): {r2_random:.3f}")
print(f"  Root Mean Squared Error (RMSE): {rmse_random:.3f}")
print("-" * 30)

print("\nInterpretation:")
print("Based on the R-squared values, which represent the proportion of the variance in the dependent variable that is predictable from the independent variables, a higher value indicates a better fit. Based on the RMSE values, which measure the average magnitude of the errors, a lower value indicates a better fit.")
print(f"The Original Model achieved an R-squared of {r2:.3f} and RMSE of {rmse:.3f}.")
print(f"The GridSearchCV Model achieved an R-squared of {r2_grid:.3f} and RMSE of {rmse_grid:.3f}.")
print(f"The RandomizedSearchCV Model achieved an R-squared of {r2_random:.3f} and RMSE of {rmse_random:.3f}.")

# Determine which model performed best
best_r2_model = "Original" if r2 >= r2_grid and r2 >= r2_random else ("GridSearchCV" if r2_grid >= r2 and r2_grid >= r2_random else "RandomizedSearchCV")
best_rmse_model = "Original" if rmse <= rmse_grid and rmse <= rmse_random else ("GridSearchCV" if rmse_grid <= rmse and rmse_grid <= rmse_random else "RandomizedSearchCV")

print(f"\nBased on R-squared, the best performing model is: {best_r2_model}")
print(f"Based on RMSE, the best performing model is: {best_rmse_model}")

"""## Summary:

### Data Analysis Key Findings

*   The best hyperparameters found by GridSearchCV for the `RandomForestRegressor` were `{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}`. The best cross-validation score obtained was approximately 0.6065.
*   The model trained with the best hyperparameters from GridSearchCV achieved an R-squared score of 0.533 and an RMSE of 0.532 on the test set.
*   The best hyperparameters found by RandomizedSearchCV were `{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': None}`. The best cross-validation score achieved was approximately 0.6016.
*   The model trained with the best hyperparameters from RandomizedSearchCV achieved an R-squared score of 0.546 and an RMSE of 0.524 on the test set.
*   Comparing the three models on the test set:
    *   Original Model: R-squared = 0.517, RMSE = 0.541
    *   GridSearchCV Model: R-squared = 0.533, RMSE = 0.532
    *   RandomizedSearchCV Model: R-squared = 0.546, RMSE = 0.524
*   Based on both R-squared (higher is better) and RMSE (lower is better), the RandomizedSearchCV model performed the best on the test set.

### Insights or Next Steps

*   Both GridSearchCV and RandomizedSearchCV improved the model's performance compared to the original model, indicating the value of hyperparameter tuning.
*   RandomizedSearchCV found a set of hyperparameters that resulted in slightly better performance on the test set than GridSearchCV in this specific case, potentially due to exploring a larger hyperparameter space more efficiently within the given iterations.

"""

from sklearn.model_selection import KFold, cross_val_score, LeaveOneOut, LeavePOut

# Re-run the train-test split just in case
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Get the best estimators from the previous steps
# Assuming grid_search and random_search objects are still available and fitted
best_grid_model = grid_search.best_estimator_
best_random_model = random_search.best_estimator_

# --- Evaluate using K-Fold Cross-Validation ---
print("\nEvaluating models using K-Fold Cross-Validation (k=5)...")
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Evaluate the original model
scores_original_kfold = cross_val_score(model, X, y, cv=kfold, scoring='r2', n_jobs=-1)
print(f"Original Model K-Fold R²: {scores_original_kfold.mean():.3f} (+/- {scores_original_kfold.std():.3f})")

# Evaluate the GridSearchCV best model
scores_grid_kfold = cross_val_score(best_grid_model, X, y, cv=kfold, scoring='r2', n_jobs=-1)
print(f"GridSearchCV Model K-Fold R²: {scores_grid_kfold.mean():.3f} (+/- {scores_grid_kfold.std():.3f})")

# Evaluate the RandomizedSearchCV best model
scores_random_kfold = cross_val_score(best_random_model, X, y, cv=kfold, scoring='r2', n_jobs=-1)
print(f"RandomizedSearchCV Model K-Fold R²: {scores_random_kfold.mean():.3f} (+/- {scores_random_kfold.std():.3f})")


# --- Evaluate using Leave-One-Out Cross-Validation (LOOCV) ---
# LOOCV can be computationally expensive for large datasets.
if len(X) < 500: # Arbitrary threshold, adjust based on computational resources
    print("\nEvaluating models using Leave-One-Out Cross-Validation (LOOCV)...")
    loocv = LeaveOneOut()

    # Evaluate the original model
    scores_original_loocv = cross_val_score(model, X, y, cv=loocv, scoring='r2', n_jobs=-1)
    print(f"Original Model LOOCV R²: {scores_original_loocv.mean():.3f}")

    # Evaluate the GridSearchCV best model
    scores_grid_loocv = cross_val_score(best_grid_model, X, y, cv=loocv, scoring='r2', n_jobs=-1)
    print(f"GridSearchCV Model LOOCV R²: {scores_grid_loocv.mean():.3f}")

    # Evaluate the RandomizedSearchCV best model
    scores_random_loocv = cross_val_score(best_random_model, X, y, cv=loocv, scoring='r2', n_jobs=-1)
    print(f"RandomizedSearchCV Model LOOCV R²: {scores_random_loocv.mean():.3f}")
    loocv_original_r2 = scores_original_loocv.mean()
    loocv_grid_r2 = scores_grid_loocv.mean()
    loocv_random_r2 = scores_random_loocv.mean()

else:
    print(f"\nSkipping LOOCV due to large dataset size ({len(X)} samples).")
    loocv_original_r2 = "N/A"
    loocv_grid_r2 = "N/A"
    loocv_random_r2 = "N/A"


# --- Evaluate using Leave-P-Out Cross-Validation (LPOCV) ---
# LPOCV is even more computationally expensive than LOOCV.
if len(X) < 100:
     print("\nEvaluating models using Leave-P-Out Cross-Validation (LPOCV, p=2)...")
     lpocv = LeavePOut(p=2) # Example with p=2

     # Evaluate the original model
     scores_original_lpocv = cross_val_score(model, X, y, cv=lpocv, scoring='r2', n_jobs=-1)
     print(f"Original Model LPOCV R²: {scores_original_lpocv.mean():.3f}")

     # Evaluate the GridSearchCV best model
     scores_grid_lpocv = cross_val_score(best_grid_model, X, y, cv=lpocv, scoring='r2', n_jobs=-1)
     print(f"GridSearchCV Model LPOCV R²: {scores_grid_lpocv.mean():.3f}")

     # Evaluate the RandomizedSearchCV best model
     scores_random_lpocv = cross_val_score(best_random_model, X, y, cv=lpocv, scoring='r2', n_jobs=-1)
     print(f"RandomizedSearchCV Model LPOCV R²: {scores_random_lpocv.mean():.3f}")
     lpocv_original_r2 = scores_original_lpocv.mean()
     lpocv_grid_r2 = scores_grid_lpocv.mean()
     lpocv_random_r2 = scores_random_lpocv.mean()
else:
    print(f"\nSkipping LPOCV due to large dataset size ({len(X)} samples).")
    lpocv_original_r2 = "N/A"
    lpocv_grid_r2 = "N/A"
    lpocv_random_r2 = "N/A"
# You can add more metrics (like negative MSE for RMSE comparison) to cross_val_score
# e.g., cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1)
# and then take the negative square root of the mean score.

print("\nComparison of Validation Results:")
print("Model             | K-Fold (R²) Mean | LOOCV (R²) | LPOCV (R²)") # Add more columns if other metrics are used
print("------------------|-----------------|------------|------------")

# Safely format the values, checking if they are strings (N/A) before applying float formatting
original_kfold_r2_mean = scores_original_kfold.mean() if scores_original_kfold is not None else "N/A"
grid_kfold_r2_mean = scores_grid_kfold.mean() if scores_grid_kfold is not None else "N/A"
random_kfold_r2_mean = scores_random_kfold.mean() if scores_random_kfold is not None else "N/A"

original_loocv_r2 = scores_original_loocv.mean() if 'scores_original_loocv' in locals() and scores_original_loocv is not None else "N/A"
grid_loocv_r2 = scores_grid_loocv.mean() if 'scores_grid_loocv' in locals() and scores_grid_loocv is not None else "N/A"
random_loocv_r2 = scores_random_loocv.mean() if 'scores_random_loocv' in locals() and scores_random_loocv is not None else "N/A"

original_lpocv_r2 = scores_original_lpocv.mean() if 'scores_original_lpocv' in locals() and scores_original_lpocv is not None else "N/A"
grid_lpocv_r2 = scores_grid_lpocv.mean() if 'scores_grid_lpocv' in locals() and scores_grid_lpocv is not None else "N/A"
random_lpocv_r2 = scores_random_lpocv.mean() if 'scores_random_lpocv' in locals() and scores_random_lpocv is not None else "N/A"


print(f"Original          | {original_kfold_r2_mean:<15.3f}" if not isinstance(original_kfold_r2_mean, str) else f"Original          | {original_kfold_r2_mean:<15s}", end=" | ")
print(f"{original_loocv_r2:<10.3f}" if not isinstance(original_loocv_r2, str) else f"{original_loocv_r2:<10s}", end=" | ")
print(f"{original_lpocv_r2:<10.3f}" if not isinstance(original_lpocv_r2, str) else f"{original_lpocv_r2:<10s}")


print(f"GridSearchCV      | {grid_kfold_r2_mean:<15.3f}" if not isinstance(grid_kfold_r2_mean, str) else f"GridSearchCV      | {grid_kfold_r2_mean:<15s}", end=" | ")
print(f"{grid_loocv_r2:<10.3f}" if not isinstance(grid_loocv_r2, str) else f"{grid_loocv_r2:<10s}", end=" | ")
print(f"{grid_lpocv_r2:<10.3f}" if not isinstance(grid_lpocv_r2, str) else f"{grid_lpocv_r2:<10s}")


print(f"RandomizedSearchCV| {random_kfold_r2_mean:<15.3f}" if not isinstance(random_kfold_r2_mean, str) else f"RandomizedSearchCV| {random_kfold_r2_mean:<15s}", end=" | ")
print(f"{random_loocv_r2:<10.3f}" if not isinstance(random_loocv_r2, str) else f"{random_loocv_r2:<10s}", end=" | ")
print(f"{random_lpocv_r2:<10.3f}" if not isinstance(random_lpocv_r2, str) else f"{random_lpocv_r2:<10s}")

feature_importances = best_random_model.feature_importances_ # Using the best random model

# Get the names of the features (fingerprint bits)
# The column names of the fingerprint DataFrame X are the feature names
feature_names = X.columns

# Create a pandas Series to easily sort importances
importances_series = pd.Series(feature_importances, index=feature_names)

# Sort the importances in descending order
sorted_importances = importances_series.sort_values(ascending=False)

# Print the top N most important features (e.g., top 20)
top_n = 20
print(f"\nTop {top_n} Most Important Features (Fingerprint Bits):")
print(sorted_importances.head(top_n))

# Optional: Visualize the top N feature importances
plt.figure(figsize=(12, 8))
sorted_importances.head(top_n).plot(kind='bar')
plt.title(f'Top {top_n} Feature Importances from Best RandomForestRegressor')
plt.xlabel('Fingerprint Bit Index')
plt.ylabel('Importance')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Using calculated descriptors (df_descriptors) as features for a more interpretable model

if 'df_descriptors' in globals() and 'df_preprocessed' in globals():

    # Ensure df_descriptors and df_preprocessed are aligned after any row dropping
    # Re-calculate pIC50 on df_preprocessed to be sure it's up-to-date after potential drops
    def calculate_pIC50(df_input):
        df = df_input.copy()
        df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')
        df.dropna(subset=['standard_value'], inplace=True)
        norm_values = []
        for i in df['standard_value']:
            if i > 100000000:
                i = 100000000
            norm_values.append(i)
        df['standard_value_norm'] = norm_values
        df['pIC50'] = -np.log10(df['standard_value_norm'] * (10**-9))
        df.drop(columns=['standard_value_norm'], inplace=True)
        return df

    df_preprocessed_for_descriptors = calculate_pIC50(df_preprocessed.copy())

    # Align df_descriptors to df_preprocessed_for_descriptors
    # This assumes df_descriptors was generated from the same smiles_list as the *initial* df_preprocessed
    # Need to re-generate descriptors on the *current* df_preprocessed after any cleaning
    print("Regenerating descriptors on processed data for alignment...")
    df_descriptors_aligned = calculate_all_rdkit_descriptors(df_preprocessed_for_descriptors['canonical_smiles'])

    # Drop columns with any NaN (from failed descriptor calculations for some molecules)
    df_descriptors_aligned = df_descriptors_aligned.dropna(axis=1, how='any')
    # Drop columns with no variance
    df_descriptors_aligned = df_descriptors_aligned.loc[:, (df_descriptors_aligned != df_descriptors_aligned.iloc[0]).any()]

    X_descriptors = df_descriptors_aligned
    y_descriptors = df_preprocessed_for_descriptors['pIC50']

    # Drop rows with NaN values in X_descriptors after generating descriptors
    nan_rows_X_desc = X_descriptors.isnull().any(axis=1)
    if nan_rows_X_desc.any():
        print(f"Dropping {nan_rows_X_desc.sum()} rows due to NaN values in descriptors.")
        X_descriptors = X_descriptors[~nan_rows_X_desc].reset_index(drop=True)
        y_descriptors = y_descriptors[~nan_rows_X_desc].reset_index(drop=True)


    print(f"\nShape of feature matrix (X_descriptors): {X_descriptors.shape}")
    print(f"Shape of target vector (y_descriptors): {y_descriptors.shape}")

    # Split data for the descriptor-based model
    X_train_desc, X_test_desc, y_train_desc, y_test_desc = train_test_split(
        X_descriptors, y_descriptors, test_size=0.2, random_state=42
    )
    print(f"Training set size (descriptors): {X_train_desc.shape[0]}, Test set size (descriptors): {X_test_desc.shape[0]}")


    # Train a RandomForestRegressor model using descriptors
    descriptor_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    descriptor_model.fit(X_train_desc, y_train_desc)

    # Evaluate the descriptor-based model
    y_pred_desc = descriptor_model.predict(X_test_desc)

    r2_desc = r2_score(y_test_desc, y_pred_desc)
    rmse_desc = np.sqrt(mean_squared_error(y_test_desc, y_pred_desc))

    print("\nDescriptor-based Model Performance on Test Set:")
    print(f"R-squared (R²): {r2_desc:.3f}")
    print(f"Root Mean Squared Error (RMSE): {rmse_desc:.3f}")

    # Extract and print feature importances for the descriptor-based model
    descriptor_importances = descriptor_model.feature_importances_
    descriptor_names = X_descriptors.columns

    importances_desc_series = pd.Series(descriptor_importances, index=descriptor_names)
    sorted_importances_desc = importances_desc_series.sort_values(ascending=False)

    print(f"\nTop {top_n} Most Important Features (RDKit Descriptors):")
    print(sorted_importances_desc.head(top_n))

    # Visualize the top N descriptor importances
    plt.figure(figsize=(14, 8)) # Increased figure size
    sorted_importances_desc.head(top_n).plot(kind='bar')
    plt.title(f'Top {top_n} Feature Importances from Descriptor-based RandomForestRegressor')
    plt.xlabel('RDKit Descriptor')
    plt.ylabel('Importance')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

    # Compare the descriptor-based model performance with fingerprint-based models
    print("\nPerformance Comparison (including Descriptor Model):")
    print("-" * 40)
    print(f"Original Fingerprint Model:")
    print(f"  R-squared (R²): {r2:.3f}")
    print(f"  Root Mean Squared Error (RMSE): {rmse:.3f}")
    print("-" * 40)
    print(f"GridSearchCV Fingerprint Model:")
    print(f"  R-squared (R²): {r2_grid:.3f}")
    print(f"  Root Mean Squared Error (RMSE): {rmse_grid:.3f}")
    print("-" * 40)
    print(f"RandomizedSearchCV Fingerprint Model:")
    print(f"  R-squared (R²): {r2_random:.3f}")
    print(f"  Root Mean Squared Error (RMSE): {rmse_random:.3f}")
    print("-" * 40)
    print(f"Descriptor-based Model:")
    print(f"  R-squared (R²): {r2_desc:.3f}")
    print(f"  Root Mean Squared Error (RMSE): {rmse_desc:.3f}")
    print("-" * 40)

    print("\nConclusion on Feature Interpretation:")
    print("The descriptor-based model provides feature importances for chemically intuitive properties (like Molecular Weight, LogP, etc.), making it easier to understand which properties contribute most to the predicted activity.")
    print("In contrast, interpreting the importance of individual fingerprint bits is challenging without knowing the exact substructure mapped to each bit, which is not directly available from the standard RDKit output for Morgan fingerprints.")


else:
    print("\nSkipping descriptor-based analysis. Ensure 'df_descriptors' and 'df_preprocessed' DataFrames are available.")